{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import img_as_float\n",
    "from torchvision import transforms\n",
    "import os\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "\n",
    "# --- Single Sequence Evaluation Setup ---\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_sequences(root_dir, stack_size=5):\n",
    "    sequences = []\n",
    "    for subject in os.listdir(root_dir):\n",
    "        subject_path = os.path.join(root_dir, subject)\n",
    "        if not os.path.isdir(subject_path):\n",
    "            continue\n",
    "        for label in os.listdir(subject_path):\n",
    "            label_path = os.path.join(subject_path, label)\n",
    "            if not os.path.isdir(label_path):\n",
    "                continue\n",
    "            for action in os.listdir(label_path):\n",
    "                action_path = os.path.join(label_path, action)\n",
    "                if not os.path.isdir(action_path):\n",
    "                    continue\n",
    "                image_paths = sorted(glob(os.path.join(action_path, '*.jpg')))\n",
    "                # Pastikan cukup frame untuk stacking\n",
    "                if len(image_paths) < stack_size:\n",
    "                    continue\n",
    "                # Ekstrak nama aksi tanpa angka depan jika ada\n",
    "                action_name = \"_\".join(action.split('_')[1:]) if '_' in action else action\n",
    "                sequences.append({\n",
    "                    'subject': subject,\n",
    "                    'label': label,\n",
    "                    'action': action_name,\n",
    "                    'image_paths': image_paths\n",
    "                })\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation n Image Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "def apply_transformations(sequences, transform, stack_size=5):\n",
    "    transformed_sequences = []\n",
    "    for seq in sequences:\n",
    "        transformed_images = []\n",
    "        for img_path in seq['image_paths']:\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if img is None:\n",
    "                continue  # Skip jika gambar tidak terbaca\n",
    "            img = np.expand_dims(img, axis=2)  # Tambahkan dimensi channel\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)  # Ubah grayscale ke RGB\n",
    "            img = np.array(img, dtype=np.uint8)\n",
    "            augmented_img = transform(img)\n",
    "            transformed_images.append(augmented_img)\n",
    "        # Pastikan setelah transformasi, jumlah gambar masih >= stack_size\n",
    "        if len(transformed_images) >= stack_size:\n",
    "            transformed_sequences.append({\n",
    "                'subject': seq['subject'],\n",
    "                'label': seq['label'],\n",
    "                'action': seq['action'],\n",
    "                'image_tensors': transformed_images\n",
    "            })\n",
    "    return transformed_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = 'train'  # Ganti dengan path sebenarnya\n",
    "sequences = load_image_sequences(root_dir, stack_size=5)\n",
    "\n",
    "# Memisahkan sequences berdasarkan label\n",
    "fall_sequences = [s for s in sequences if s['label'] == 'fall']\n",
    "non_fall_sequences = [s for s in sequences if s['label'] == 'non_fall']\n",
    "\n",
    "# Membagi data masing-masing label menjadi train dan test dengan memastikan balance\n",
    "train_fall, test_fall = train_test_split(fall_sequences, test_size=0.2, random_state=42)\n",
    "train_non_fall, test_non_fall = train_test_split(non_fall_sequences, test_size=0.2, random_state=42)\n",
    "\n",
    "# Menggabungkan train dan test set\n",
    "train_sequences = train_fall + train_non_fall\n",
    "test_sequences = test_fall + test_non_fall\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(train_sequences)\n",
    "random.shuffle(test_sequences)\n",
    "\n",
    "# Mengacak urutan data\n",
    "random.seed(42)\n",
    "random.shuffle(train_sequences)\n",
    "random.shuffle(test_sequences)\n",
    "\n",
    "# Verifikasi distribusi label\n",
    "train_labels = [s['label'] for s in train_sequences]\n",
    "test_labels = [s['label'] for s in test_sequences]\n",
    "\n",
    "print(\"Train label distribution:\", Counter(train_labels))\n",
    "print(\"Test label distribution:\", Counter(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Action Distribution:\n",
      "Action                         Count      Number of Subjects  \n",
      "------------------------------------------------------------\n",
      "3_standing_falls               1          1                   \n",
      "2_laying                       1          1                   \n",
      "4_sitting_falls                1          1                   \n",
      "4_walking                      1          1                   \n",
      "2_stretching                   1          1                   \n",
      "4_stretching                   1          1                   \n",
      "4_left_falls                   1          1                   \n",
      "1_left_falls                   1          1                   \n",
      "3_jumping                      1          1                   \n",
      "3_walking                      1          1                   \n",
      "1_squat                        1          1                   \n",
      "1_standing_falls               1          1                   \n",
      "1_sitting_falls                1          1                   \n",
      "2_squat                        1          1                   \n",
      "4_right_falls                  1          1                   \n",
      "1_picking                      1          1                   \n",
      "3_laying                       1          1                   \n",
      "4_squat                        1          1                   \n",
      "3_left_falls                   1          1                   \n",
      "2_jumping                      1          1                   \n",
      "4_picking                      1          1                   \n",
      "3_squat                        1          1                   \n",
      "2_right_falls                  1          1                   \n",
      "4_laying                       1          1                   \n",
      "4_standing_falls               1          1                   \n",
      "2_backward_falls               1          1                   \n",
      "1_forward_falls                1          1                   \n",
      "3_picking                      1          1                   \n",
      "1_stretching                   1          1                   \n",
      "1_walking                      1          1                   \n",
      "1_laying                       1          1                   \n",
      "3_backward_falls               1          1                   \n",
      "1_right_falls                  1          1                   \n",
      "2_forward_falls                1          1                   \n",
      "2_sitting_falls                1          1                   \n",
      "4_forward_falls                1          1                   \n",
      "3_forward_falls                1          1                   \n",
      "3_right_falls                  1          1                   \n",
      "\n",
      "Test Action Distribution:\n",
      "Action                         Count      Number of Subjects  \n",
      "------------------------------------------------------------\n",
      "4_backward_falls               1          1                   \n",
      "4_jumping                      1          1                   \n",
      "2_left_falls                   1          1                   \n",
      "1_jumping                      1          1                   \n",
      "2_standing_falls               1          1                   \n",
      "1_backward_falls               1          1                   \n",
      "2_walking                      1          1                   \n",
      "2_picking                      1          1                   \n",
      "3_sitting_falls                1          1                   \n",
      "3_stretching                   1          1                   \n"
     ]
    }
   ],
   "source": [
    "def get_action_distribution(sequences):\n",
    "    action_counts = defaultdict(int)\n",
    "    action_subjects = defaultdict(set)\n",
    "    for s in sequences:\n",
    "        action = s['action']\n",
    "        action_counts[action] += 1\n",
    "        action_subjects[action].add(s['subject'])\n",
    "    return action_counts, action_subjects\n",
    "\n",
    "train_action_counts, train_action_subjects = get_action_distribution(train_sequences)\n",
    "test_action_counts, test_action_subjects = get_action_distribution(test_sequences)\n",
    "\n",
    "def print_action_distribution(action_counts, action_subjects, dataset_name):\n",
    "    print(f\"\\n{dataset_name} Action Distribution:\")\n",
    "    print(f\"{'Action':<30} {'Count':<10} {'Number of Subjects':<20}\")\n",
    "    print(\"-\" * 60)\n",
    "    for action, count in action_counts.items():\n",
    "        num_subjects = len(action_subjects[action])\n",
    "        print(f\"{action:<30} {count:<10} {num_subjects:<20}\")\n",
    "\n",
    "print_action_distribution(train_action_counts, train_action_subjects, \"Train\")\n",
    "print_action_distribution(test_action_counts, test_action_subjects, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Terapkan augmentasi pada train_sequences\n",
    "train_augmented = apply_transformations(train_sequences, augmentation, stack_size=5)\n",
    "\n",
    "# Terapkan transformasi tanpa augmentasi pada test_sequences\n",
    "test_transformed = apply_transformations(test_sequences, test_transform, stack_size=5)\n",
    "\n",
    "# --- Stacking Consecutive Frames ---\n",
    "\n",
    "def stack_consecutive_frames(sequences, stack_size=5):\n",
    "    stacked_sequences = []\n",
    "    for seq in sequences:\n",
    "        images = seq['image_tensors']\n",
    "        if len(images) < stack_size:\n",
    "            continue\n",
    "        for i in range(len(images) - stack_size + 1):\n",
    "            stacked = torch.stack(images[i:i+stack_size])  # Shape: (stack_size, C, H, W)\n",
    "            stacked_sequences.append({\n",
    "                'subject': seq['subject'],\n",
    "                'label': seq['label'],\n",
    "                'action': seq['action'],\n",
    "                'stacked_images': stacked\n",
    "            })\n",
    "    return stacked_sequences\n",
    "\n",
    "# Terapkan stacking pada train_augmented\n",
    "train_stacked = stack_consecutive_frames(train_augmented, stack_size=5)\n",
    "\n",
    "# Terapkan stacking pada test_transformed\n",
    "test_stacked = stack_consecutive_frames(test_transformed, stack_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackedActionDataset(Dataset):\n",
    "    def __init__(self, stacked_sequences):\n",
    "        self.stacked_sequences = stacked_sequences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.stacked_sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.stacked_sequences[idx]\n",
    "        images = sequence['stacked_images']  # Shape: [stack_size, C, H, W]\n",
    "        label = 1 if sequence['label'] == 'fall' else 0\n",
    "        # Rearrange to [C, stack_size, H, W] if model expects channels first\n",
    "        images = images.permute(1, 0, 2, 3)  # [C, stack_size, H, W]\n",
    "        return images, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4  # Kurangi batch size untuk menghindari penggunaan memori berlebih\n",
    "\n",
    "train_dataset = StackedActionDataset(train_stacked)\n",
    "test_dataset = StackedActionDataset(test_stacked)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv3DModel(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super(Conv3DModel, self).__init__()\n",
    "        # Menggunakan model 3D ResNet18 sebagai backbone\n",
    "        self.backbone = models.video.r3d_18(pretrained=pretrained)\n",
    "        num_ftrs = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Identity()  # Menghapus layer akhir\n",
    "\n",
    "        # Menambahkan layer klasifikasi\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 2)  # Kelas: fall dan non_fall\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, C, stack_size, H, W]\n",
    "        x = self.backbone(x)  # Output shape: [batch_size, num_ftrs]\n",
    "        x = self.classifier(x)  # Output shape: [batch_size, 2]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\envs\\venv\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ASUS\\anaconda3\\envs\\venv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=R3D_18_Weights.KINETICS400_V1`. You can also use `weights=R3D_18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/r3d_18-b3b3357e.pth\" to C:\\Users\\ASUS/.cache\\torch\\hub\\checkpoints\\r3d_18-b3b3357e.pth\n",
      "100%|██████████| 127M/127M [00:37<00:00, 3.57MB/s] \n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Conv3DModel(pretrained=True).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [64, 3, 3, 7, 7], expected input[8, 5, 3, 224, 224] to have 3 channels, but got 5 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 13\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     15\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[11], line 19\u001b[0m, in \u001b[0;36mConv3DModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# x shape: [batch_size, stack_size, C, H, W]\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Output shape: [batch_size, num_ftrs]\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(x)  \u001b[38;5;66;03m# Output shape: [batch_size, 2]\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\venv\\lib\\site-packages\\torchvision\\models\\video\\resnet.py:251\u001b[0m, in \u001b[0;36mVideoResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 251\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    253\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(x)\n\u001b[0;32m    254\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\venv\\lib\\site-packages\\torch\\nn\\modules\\container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\venv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:608\u001b[0m, in \u001b[0;36mConv3d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\venv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:603\u001b[0m, in \u001b[0;36mConv3d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    592\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv3d(\n\u001b[0;32m    593\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    594\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    601\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    602\u001b[0m     )\n\u001b[1;32m--> 603\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    604\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    605\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 3, 3, 7, 7], expected input[8, 5, 3, 224, 224] to have 3 channels, but got 5 channels instead"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "    \n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)  # Shape: [batch_size, C, stack_size, H, W]\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Calculate training accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = 100 * correct_train / total_train\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%')\n",
    "\n",
    "# --- Evaluation ---\n",
    "    \n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "print(f'Accuracy of the model on test sequences: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Modified Evaluation Function to Stack the Same Frame ---\n",
    "    \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from collections import deque\n",
    "\n",
    "class SingleFrameDataset(Dataset):\n",
    "    def __init__(self, sequences, transform=None):\n",
    "        \"\"\"\n",
    "        Dataset for single frame evaluation.\n",
    "\n",
    "        Args:\n",
    "            sequences (list): List of sequences from the test set.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.frame_paths = []\n",
    "        self.labels = []\n",
    "        for seq in sequences:\n",
    "            for img_path in seq['image_paths']:\n",
    "                self.frame_paths.append(img_path)\n",
    "                self.labels.append(1 if seq['label'] == 'fall' else 0)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.frame_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.frame_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Load image in grayscale\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is None:\n",
    "            raise ValueError(f\"Image not found or unable to read: {img_path}\")\n",
    "\n",
    "        # Convert grayscale to RGB\n",
    "        img = np.expand_dims(img, axis=2)  # [H, W, 1]\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)  # [H, W, 3]\n",
    "        img = np.array(img, dtype=np.uint8)\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)  # [C, H, W]\n",
    "\n",
    "        return img, label\n",
    "\n",
    "# Define transformations similar to training\n",
    "single_frame_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # Sesuaikan untuk RGB\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create SingleFrameDataset\n",
    "single_frame_test_dataset = SingleFrameDataset(test_sequences, transform=single_frame_transform)\n",
    "\n",
    "# Create DataLoader with batch_size=1 dan tanpa shuffle\n",
    "single_frame_test_loader = DataLoader(single_frame_test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# --- Modified Evaluation Function ---\n",
    "\n",
    "def evaluate_single_frame(model, single_frame_loader, device, stack_size=5):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, labels) in enumerate(single_frame_loader):\n",
    "            # Debugging: Cetak informasi batch\n",
    "            print(f'Processing batch {batch_idx + 1}')\n",
    "\n",
    "            # images: [batch_size=1, C, H, W]\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            frame = images[0]  # [C, H, W]\n",
    "\n",
    "            # Membuat stack dengan mengulangi frame yang sama\n",
    "            stacked_frames = frame.unsqueeze(1).repeat(1, stack_size, 1, 1)  # [C, stack_size, H, W]\n",
    "\n",
    "            # Menambahkan dimensi batch: [1, C, stack_size, H, W]\n",
    "            stacked_frames = stacked_frames.unsqueeze(0)\n",
    "\n",
    "            # Debugging: Cetak bentuk tensor\n",
    "            print(f'Stacked Frames Shape: {stacked_frames.shape}')  # Harusnya [1, C, stack_size, H, W]\n",
    "\n",
    "            # Pengecekan dimensi\n",
    "            assert stacked_frames.dim() == 5, f\"Expected 5D tensor, got {stacked_frames.dim()}D tensor\"\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(stacked_frames)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            # Debugging: Tampilkan prediksi dan label\n",
    "            print(f'Predicted: {predicted.item()}, Actual: {labels.item()}')\n",
    "\n",
    "            total += 1\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total if total > 0 else 0\n",
    "    print(f'Accuracy of the model on single frame test sequences: {accuracy:.2f}%')\n",
    "\n",
    "# --- Modified Evaluation Call ---\n",
    "\n",
    "evaluate_single_frame(model, single_frame_test_loader, device, stack_size=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
